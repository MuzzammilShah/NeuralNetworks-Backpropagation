## SET 1 - MICROGRAD ğŸ”—

[![Documentation](https://img.shields.io/badge/Documentation-Available-blue)](https://muzzammilshah.github.io/Road-to-GPT/Micrograd/)
![Number of Commits](https://img.shields.io/github/commit-activity/m/MuzzammilShah/NeuralNetworks-Micrograd-Implementation?label=Commits)
[![Last Commit](https://img.shields.io/github/last-commit/MuzzammilShah/NeuralNetworks-Micrograd-Implementation.svg?style=flat)](https://github.com/MuzzammilShah/NeuralNetworks-Micrograd-Implementation/commits/main)  
![Project Status](https://img.shields.io/badge/Status-Done-success)

&nbsp;

### **Overview**
This repository contains the implementation of **Backpropagation** using an **AutoGrad Engine**, inspired by the **Micrograd** video by Andrej Karpathy. It explores the foundations of training neural networks and implementing key operations from scratch.

The repository contains:

- **Manual Backpropagation**: Building intuition and understanding of the gradient calculation process.
- **Interactive Site Version**: A pilot version of an interactive site that visualizes the functionality, currently under development.

âœğŸ» Notes: Follow the notebooks in order for a structured learning path. Each notebook and note corresponds to a particular concept or milestone in the implementation.

&nbsp;

### **ğŸ—‚ï¸Repository Structure**

```plaintext
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ notes/
â”‚   â”œâ”€â”€ A-main-video-lecture-notes.md
â”‚   â”œâ”€â”€ chatgpt-motivation.md
â”‚   â”œâ”€â”€ crux-node-backpropagation.md
â”‚   â”œâ”€â”€ expanding-tanh-and-adding-more-operations.md
â”‚   â”œâ”€â”€ micrograd-functionality.md
â”‚   â”œâ”€â”€ multi-layer-perceptron.md
â”‚   â”œâ”€â”€ neurons-explanation.md
â”‚   â”œâ”€â”€ pytorch-comparision.md
â”‚   â””â”€â”€ value-object-creation.md
â”œâ”€â”€ site/
â”‚   â”œâ”€â”€ interactive_site_pilot_v1.2/
â”œâ”€â”€ 1-derivative-simple-function.ipynb
â”œâ”€â”€ 2-derivative-function-with-multiple-inputs.ipynb
â”œâ”€â”€ 3-value-object.ipynb
â”œâ”€â”€ 3_1-graph-visualisation.ipynb
â”œâ”€â”€ 4_0-manual-backpropagation_simpleExpression.ipynb
â”œâ”€â”€ ... (remaining implementation notebooks)
```

- **Notes Directory**: Contains Markdown files with notes and explanations for each topic.
- **Interactive Site Directory**: Contains files for the pilot version of the interactive visualization tool.
- **Implementation Notebooks**: Step-by-step code for implementing and understanding backpropagation and related concepts.

&nbsp;

### **ğŸ“„Instructions**

1. Start by reading the notes in the `notes/` directory for a theoretical understanding.
2. Proceed with the notebooks in the root directory in order to build up the implementation step by step.
3. Explore the `site/` directory for the pilot interactive version of the AutoGrad Engine visualization (Idea concept, not yet implemented)

&nbsp;

### **â­Documentation**

For a better reading experience and detailed notes, visit my **[Road to GPT Documentation Site](https://muzzammilshah.github.io/Road-to-GPT/Micrograd/)**. 

> **ğŸ’¡Pro Tip**: This site provides an interactive and visually rich explanation of the notes and code. It is highly recommended you view this project from there.

&nbsp;

### **âœğŸ»Acknowledgments**
Notes and implementations inspired by the **Micrograd** video by [Andrej Karpathy](https://karpathy.ai/).  

For more of my projects, visit my [Portfolio Site](https://muhammedshah.com).